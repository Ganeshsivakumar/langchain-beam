"use strict";(globalThis.webpackChunkdocs_site=globalThis.webpackChunkdocs_site||[]).push([[422],{5438:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>o,contentTitle:()=>r,default:()=>h,frontMatter:()=>l,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"templates/kafka-to-helixdb","title":"Kafka to HelixDB","description":"The Kafka to helixdb Pipeline is a pre-built Apache Beam streaming pipeline that lets you consume real-time text data from Kafka topics, generate embeddings using OpenAI models, and store the vectors into HelixDB node for similarity search and retrieval. The pipeline automatically handles windowing, embedding generation, and upserts to HelixDb\'s endpoint.","source":"@site/docs/templates/kafka-to-helixdb.md","sourceDirName":"templates","slug":"/templates/kafka-to-helixdb","permalink":"/langchain-beam/docs/templates/kafka-to-helixdb","draft":false,"unlisted":false,"editUrl":"https://github.com/Ganeshsivakumar/langchain-beam/tree/main/docs/docs-site/docs/docs/templates/kafka-to-helixdb.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Templates","permalink":"/langchain-beam/docs/category/templates"},"next":{"title":"Kafka to Pinecone","permalink":"/langchain-beam/docs/templates/kafka-to-pinecone"}}');var a=t(4848),s=t(8453);const l={},r="Kafka to HelixDB",o={},d=[{value:"What It Does \ud83d\udee0\ufe0f",id:"what-it-does-\ufe0f",level:2},{value:"Use Cases \ud83d\udce6",id:"use-cases-",level:2},{value:"Template Parameters \u2699\ufe0f",id:"template-parameters-\ufe0f",level:2},{value:"Pipeline Architecture \ud83c\udfd7\ufe0f",id:"pipeline-architecture-\ufe0f",level:2},{value:"Kafka Cluster Authentication",id:"kafka-cluster-authentication",level:4},{value:"OpenAI API Calls and Batching",id:"openai-api-calls-and-batching",level:4},{value:"HelixDB ingestion endpoint",id:"helixdb-ingestion-endpoint",level:4},{value:"How to Run \ud83d\ude80",id:"how-to-run-",level:2},{value:"1. Google Cloud Dataflow",id:"1-google-cloud-dataflow",level:3},{value:"Run Template:",id:"run-template",level:4},{value:"2. Apache Flink",id:"2-apache-flink",level:3},{value:"Submitting Job",id:"submitting-job",level:4},{value:"How It Works",id:"how-it-works",level:4},{value:"3. LangBeam (Managed Cloud)",id:"3-langbeam-managed-cloud",level:3},{value:"<strong>Sign up for early access.</strong>",id:"sign-up-for-early-access",level:4},{value:"4. Locally",id:"4-locally",level:3},{value:"Run with Maven",id:"run-with-maven",level:4},{value:"Template Support \ud83e\uddf0",id:"template-support-",level:2}];function c(e){const n={a:"a",blockquote:"blockquote",br:"br",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"kafka-to-helixdb",children:"Kafka to HelixDB"})}),"\n",(0,a.jsxs)(n.p,{children:["The Kafka to helixdb Pipeline is a pre-built Apache Beam streaming pipeline that lets you consume real-time text data from Kafka topics, generate embeddings using OpenAI models, and store the vectors into ",(0,a.jsx)(n.a,{href:"https://www.helix-db.com/",children:"HelixDB"})," node for similarity search and retrieval. The pipeline automatically handles windowing, embedding generation, and upserts to HelixDb's endpoint."]}),"\n",(0,a.jsx)(n.h2,{id:"what-it-does-\ufe0f",children:"What It Does \ud83d\udee0\ufe0f"}),"\n",(0,a.jsx)(n.p,{children:"This template:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Consumes streaming text data from Kafka topics in real-time"}),"\n",(0,a.jsx)(n.li,{children:"Applies windowing to batch messages for efficient processing (10-second fixed windows)"}),"\n",(0,a.jsxs)(n.li,{children:["Generates embeddings using OpenAI embedding models via the ",(0,a.jsx)(n.a,{href:"https://github.com/Ganeshsivakumar/langchain-beam",children:"LangchainBeam"})," library"]}),"\n",(0,a.jsx)(n.li,{children:"Writes vector embeddings to HelixDB instance's endpoint"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"use-cases-",children:"Use Cases \ud83d\udce6"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Real-time semantic search indexing"}),"\n",(0,a.jsx)(n.li,{children:"Live recommendation systems"}),"\n",(0,a.jsx)(n.li,{children:"Real-time knowledge base updates"}),"\n",(0,a.jsx)(n.li,{children:"Event-driven vector database population"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"template-parameters-\ufe0f",children:"Template Parameters \u2699\ufe0f"}),"\n",(0,a.jsxs)(n.table,{children:[(0,a.jsx)(n.thead,{children:(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.th,{children:"Parameter"}),(0,a.jsx)(n.th,{children:"Description"}),(0,a.jsx)(n.th,{children:"Type"})]})}),(0,a.jsxs)(n.tbody,{children:[(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.code,{children:"brokers"})}),(0,a.jsxs)(n.td,{children:["Kafka bootstrap servers, comma-separated (e.g., ",(0,a.jsx)(n.code,{children:"broker_1:9092,broker_2:9092"}),")"]}),(0,a.jsx)(n.td,{children:"Required"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.code,{children:"topic"})}),(0,a.jsxs)(n.td,{children:["Kafka topic to consume messages from (e.g., ",(0,a.jsx)(n.code,{children:"text-content"}),", ",(0,a.jsx)(n.code,{children:"documents"}),")"]}),(0,a.jsx)(n.td,{children:"Required"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.code,{children:"kafkaUsername"})}),(0,a.jsx)(n.td,{children:"Kafka username or API key"}),(0,a.jsx)(n.td,{children:"Optional"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.code,{children:"kafkaPassword"})}),(0,a.jsx)(n.td,{children:"Kafka password or API secret"}),(0,a.jsx)(n.td,{children:"Optional"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.code,{children:"kafkaSecurityProtocol"})}),(0,a.jsxs)(n.td,{children:["Security protocol: ",(0,a.jsx)(n.code,{children:"PLAINTEXT"}),", ",(0,a.jsx)(n.code,{children:"SSL"}),", ",(0,a.jsx)(n.code,{children:"SASL_PLAINTEXT"}),", ",(0,a.jsx)(n.code,{children:"SASL_SSL"})]}),(0,a.jsx)(n.td,{children:"Optional"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.code,{children:"kafkaSaslMechanism"})}),(0,a.jsxs)(n.td,{children:["SASL mechanism: ",(0,a.jsx)(n.code,{children:"PLAIN"}),", ",(0,a.jsx)(n.code,{children:"SCRAM-SHA-256"}),", ",(0,a.jsx)(n.code,{children:"SCRAM-SHA-512"})]}),(0,a.jsx)(n.td,{children:"Optional"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.code,{children:"embeddingModel"})}),(0,a.jsxs)(n.td,{children:["OpenAI embedding model name (e.g., ",(0,a.jsx)(n.code,{children:"text-embedding-3-small"}),")"]}),(0,a.jsx)(n.td,{children:"Required"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.code,{children:"openaiApiKey"})}),(0,a.jsxs)(n.td,{children:["Your OpenAI API key for embedding generation (e.g., ",(0,a.jsx)(n.code,{children:"sk-proj-..."}),")"]}),(0,a.jsx)(n.td,{children:"Required"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.code,{children:"helixEndpoint"})}),(0,a.jsx)(n.td,{children:"HelixDB query endpoint"}),(0,a.jsx)(n.td,{children:"Required"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.code,{children:"helixUrl"})}),(0,a.jsx)(n.td,{children:"Host URL of your HelixDB instance"}),(0,a.jsx)(n.td,{children:"Required"})]})]})]}),"\n",(0,a.jsx)(n.h2,{id:"pipeline-architecture-\ufe0f",children:"Pipeline Architecture \ud83c\udfd7\ufe0f"}),"\n",(0,a.jsxs)(n.p,{children:["This pipeline continuously consumes messages from a Kafka topic using Apache Beam\u2019s KafkaIO connector. It\u2019s set up to read new messages arriving after the pipeline starts. The pipeline currently expects both message keys and values as plain strings, deserialized using Kafka\u2019s ",(0,a.jsx)(n.code,{children:"StringDeserializer"}),". Incoming messages are grouped into fixed 10-second windows for batch processing and limit the frequency of calls to external services like OpenAI and HelixDB, helping to prevent overload. Each message\u2019s content is sent to an OpenAI embedding model via LangchainBeam, which generates vector embeddings. These embeddings are then upserted into a HelixDB query endpoint"]}),"\n",(0,a.jsx)(n.p,{children:"some kafka clusters might be setup up with different serialization formats such as Avro, which typically requires integration with a schema registry to manage schemas, others may use different Kafka authentication mechanism. While these are not yet supported in this template, we plan to add them soon. We welcome your feature requests and contributions; please open an issue on GitHub repository to share your ideas."}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"kafka-to-helixdb Architecture",src:t(6557).A+"",width:"1553",height:"788"})}),"\n",(0,a.jsx)(n.h4,{id:"kafka-cluster-authentication",children:"Kafka Cluster Authentication"}),"\n",(0,a.jsxs)(n.p,{children:["If your kafka cluster is managed service like confluent cloud, Amazon MSK then kafka connector in pipeline needs to authenticate with the brokers to read data from topics. The pipeline supports standard Kafka authentication parameters (",(0,a.jsx)(n.code,{children:"kafkaUsername"}),", ",(0,a.jsx)(n.code,{children:"kafkaPassword"}),", ",(0,a.jsx)(n.code,{children:"kafkaSecurityProtocol"}),", and ",(0,a.jsx)(n.code,{children:"kafkaSaslMechanism"}),") for secure connection. If authentication parameters are not provided, the template defaults to connecting without authentication."]}),"\n",(0,a.jsx)(n.h4,{id:"openai-api-calls-and-batching",children:"OpenAI API Calls and Batching"}),"\n",(0,a.jsx)(n.p,{children:"Currently, even though the pipeline groups messages into fixed 10-second windows to control processing frequency, it still sends embedding requests individually for each message within those windows. This means the number of OpenAI API calls scales with message volume, which may impact cost and rate limits."}),"\n",(0,a.jsx)(n.p,{children:"In future updates, we plan to implement true batch embedding calls\u2014sending multiple messages in a single API request\u2014to significantly reduce the number of calls, lower costs."}),"\n",(0,a.jsx)(n.h4,{id:"helixdb-ingestion-endpoint",children:"HelixDB ingestion endpoint"}),"\n",(0,a.jsxs)(n.p,{children:["At present the pipeline the supports writing the content and its embeddings into the endpoint So, make sure that your helix query(endpoint) that you'll be passing in pipeline options doesn't take any additional properties or metadata. Also the name the parameters as ",(0,a.jsx)(n.code,{children:"vector"})," and ",(0,a.jsx)(n.code,{children:"content"})," with its data types."]}),"\n",(0,a.jsx)(n.p,{children:"Example ingestion query:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"QUERY InsertVector (vector: [F64], content: String) =>\n    \n    document <- AddV<Document>(vector, { content: content })\n    RETURN document\n"})}),"\n",(0,a.jsx)(n.h2,{id:"how-to-run-",children:"How to Run \ud83d\ude80"}),"\n",(0,a.jsx)(n.p,{children:"You can deploy this streaming pipeline using the Beam runner of your choice."}),"\n",(0,a.jsx)(n.h3,{id:"1-google-cloud-dataflow",children:"1. Google Cloud Dataflow"}),"\n",(0,a.jsxs)(n.p,{children:["This pipeline is built and packaged as a ",(0,a.jsx)(n.a,{href:"https://cloud.google.com/dataflow/docs/guides/templates/using-flex-templates",children:"Dataflow Flex Template"}),", and the template file (",(0,a.jsx)(n.code,{children:"gs://langbeam-cloud/templates/kafka-to-helixdb.json"}),") is publicly accessible. This means you can run it directly in your own GCP project by using the gcloud CLI with appropriate parameters."]}),"\n",(0,a.jsxs)(n.p,{children:["The pipeline source code is fully ",(0,a.jsx)(n.a,{href:"https://github.com/Ganeshsivakumar/langchain-beam",children:"open source on GitHub"}),". You're free to fork the repository, customize the pipeline, rebuild it as a Flex Template, and deploy it using your own GCP infrastructure."]}),"\n",(0,a.jsx)(n.h4,{id:"run-template",children:"Run Template:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'gcloud dataflow flex-template run "kafka-to-helixdb-stream" \\\n  --template-file-gcs-location="gs://langbeam-cloud/templates/kafka-to-helixdb.json" \\\n  --region="us-east1" \\\n  --project="your-project-id" \\\n  --network="default" \\\n  --subnetwork="https://www.googleapis.com/compute/v1/projects/project-id/regions/us-east1/subnetworks/default" \\\n  --staging-location="gs://your-stage-bucket/stage/" \\\n  --temp-location="gs://your-stage-bucket/temp/" \\\n  --parameters="brokers=your-kafka-cluster:9092,topic=text-content,kafkaUsername=api_key,kafkaPassword=password,kafkaSecurityProtocol=SASL_SSL,kafkaSaslMechanism=PLAIN,embeddingModel=text-embedding-3-small,openaiApiKey=your_openai_key,helixEndpoint=InsertVector,helixUrl=https://host-name:6969"\n'})}),"\n",(0,a.jsx)(n.p,{children:"If you'd like to host the template in your own GCP project:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Fork the Langchain-Beam repo"})," and clone it locally.",(0,a.jsx)(n.br,{}),"\n","You\u2019ll find the ",(0,a.jsx)(n.code,{children:"kafka-to-helixdb"})," template under the ",(0,a.jsx)(n.code,{children:"templates/"})," directory."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Build the template"})," using Maven:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"mvn clean package -Prunner-dataflow\n"})}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsxs)(n.strong,{children:["Use the ",(0,a.jsx)(n.code,{children:"gcloud"})," CLI"]})," to run Dataflow Flex Template Build command.",(0,a.jsx)(n.br,{}),"\n","This will:"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Push your pipeline JAR as a container image to Artifact Registry"}),"\n",(0,a.jsxs)(n.li,{children:["Generate a ",(0,a.jsx)(n.code,{children:"template.json"})," file in your GCS bucket, which acts as a pointer to your pipeline image and contains configuration metadata"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"gcloud dataflow flex-template build gs://your-bucket/templates/kafka-to-helixdb.json \\\n  --image-gcr-path=us-docker.pkg.dev/your-project/your-repo/kafka-to-helixdb:latest \\\n  --jar=/your-folder-path/langchain-beam/templates/kafka-to-helixdb/target/kafka-to-helixdb-dataflow.jar \\\n  --env=FLEX_TEMPLATE_JAVA_MAIN_CLASS=com.templates.langchainbeam.KafkaToHelixDb \\\n  --flex-template-base-image=JAVA17 \\\n  --metadata-file=/your-folder-path/langchain-beam/templates/kafka-to-helixdb/src/main/metadata/metadata.json \\\n  --sdk-language=JAVA\n"})}),"\n",(0,a.jsx)(n.p,{children:"Now the template is built and hosted your GCS path. you can pass GCS template file to run command to execute the template on dataflow"}),"\n",(0,a.jsx)(n.h3,{id:"2-apache-flink",children:"2. Apache Flink"}),"\n",(0,a.jsx)(n.p,{children:"If you have an Apache Flink standalone cluster, you can submit the template as a job using the prebuilt Docker image."}),"\n",(0,a.jsxs)(n.blockquote,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Note:"})," Don\u2019t have a remote Flink cluster? No problem!\nYou can quickly spin up Flink on your laptop in just ",(0,a.jsx)(n.a,{href:"https://ganeshsivakumar.github.io/langchain-beam/docs/tutorials/setup-flink/",children:"3 simple steps"})," and run templates locally."]}),"\n"]}),"\n",(0,a.jsx)(n.h4,{id:"submitting-job",children:"Submitting Job"}),"\n",(0,a.jsxs)(n.p,{children:["Once your Flink cluster is up and running, you can submit a template as a job using Docker.",(0,a.jsx)(n.br,{}),"\n","Run the following command, adjusting the pipeline options as needed:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"docker run --rm \\\n  -e FLINK_MASTER=host.docker.internal:8081 \\\n  -e FLINK_VERSION=1.18 \\\n  us-docker.pkg.dev/ptransformers/langbeam-cloud/templates/flink/kafka-to-helixdb:latest \\\n  --runner=FlinkRunner \\\n  --brokers=your-kafka-cluster:9092 \\\n  --topic=text-content \\\n  --kafkaUsername=key \\\n  --kafkaPassword=secret \\\n  --kafkaSecurityProtocol=SASL_PLAINTEXT \\\n  --kafkaSaslMechanism=PLAIN \\\n  --embeddingModel=text-embedding-3-small \\\n  --openaiApiKey=your_openai_key \\\n  --helixEndpoint=InsertVector \\\n  --helixUrl=https://host-name:6969\n"})}),"\n",(0,a.jsx)(n.h4,{id:"how-it-works",children:"How It Works"}),"\n",(0,a.jsxs)(n.p,{children:["The pipeline is built and packaged as a JAR. Since Apache Beam\u2019s Flink Runner must be compatible with your Flink version, there are multiple JARs available\u2014each tailored to a specific Beam and Flink version combination.\nRefer to the Flink version ",(0,a.jsx)(n.a,{href:"https://beam.apache.org/documentation/runners/flink/#flink-version-compatibility",children:"compatibility matrix"})," to choose the correct version."]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["The container downloads the appropriate .jar file from GCS based on ",(0,a.jsx)(n.em,{children:(0,a.jsx)(n.strong,{children:"FLINK_VERSION"})})," (your flink cluster version) with correct beam and runner dependencies"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["It uses the Flink CLI (flink run) to submit the job to the Flink cluster (as specified by ",(0,a.jsx)(n.em,{children:(0,a.jsx)(n.strong,{children:"FLINK_MASTER"})})," - Flink cluster Url )"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"All dependencies\u2014including Java 17 and the Flink CLI\u2014are preinstalled in the image, so you don\u2019t need to set up anything else."}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"3-langbeam-managed-cloud",children:"3. LangBeam (Managed Cloud)"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.a,{href:"https://www.langbeam.cloud/",children:(0,a.jsx)(n.strong,{children:"LangBeam"})})," is a fully managed platform for running Apache Beam pipelines, such as this Kafka-to-helixdb template. Instead of dealing with infrastructure setup, runner configuration, provisioning resources, and scaling. You simply provide the required template parameters and start the pipeline."]}),"\n",(0,a.jsxs)(n.p,{children:["From that moment, your ",(0,a.jsx)(n.strong,{children:"AI agents and RAG applications"})," begin receiving real-time data \u2014 continuously, reliably, and at scale."]}),"\n",(0,a.jsx)(n.h4,{id:"sign-up-for-early-access",children:(0,a.jsxs)(n.strong,{children:["Sign up for ",(0,a.jsx)(n.a,{href:"https://app.youform.com/forms/9hbdmkly",children:"early access"}),"."]})}),"\n",(0,a.jsx)(n.h3,{id:"4-locally",children:"4. Locally"}),"\n",(0,a.jsx)(n.p,{children:"You can also run the template locally using the Apache Beam DirectRunner. This is useful for testing, debugging, or running small jobs without setting up a full cluster."}),"\n",(0,a.jsx)(n.p,{children:"Prerequisites:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"JDK 17"}),"\n",(0,a.jsx)(n.li,{children:"Maven"}),"\n"]}),"\n",(0,a.jsx)(n.h4,{id:"run-with-maven",children:"Run with Maven"}),"\n",(0,a.jsx)(n.p,{children:"Clone the repository:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"git clone https://github.com/Ganeshsivakumar/langchain-beam.git\ncd langchain-beam/templates/kafka-to-helixdb\n"})}),"\n",(0,a.jsx)(n.p,{children:"Build the template:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"mvn clean package -Prunner-direct\n"})}),"\n",(0,a.jsx)(n.p,{children:"Run with direct runner:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"java -cp target/kafka-to-helixdb-direct.jar \\\n  com.templates.langchainbeam.KafkaToHelixDb \\\n  --runner=DirectRunner \\\n  --brokers=localhost:9092 \\\n  --topic=my-topic \\\n  --kafkaUsername=key \\\n  --kafkaPassword=secret \\\n  --kafkaSecurityProtocol=SASL_PLAINTEXT \\\n  --kafkaSaslMechanism=PLAIN \\\n  --embeddingModel=text-embedding-3-small \\\n  --openaiApiKey=your_openai_key \\\n  --helixEndpoint=InsertVector \\\n  --helixUrl=https://host-name:6969\n  \n\n"})}),"\n",(0,a.jsx)(n.h2,{id:"template-support-",children:"Template Support \ud83e\uddf0"}),"\n",(0,a.jsxs)(n.p,{children:["This template is fully open source and part of the ",(0,a.jsx)(n.a,{href:"https://github.com/Ganeshsivakumar/langchain-beam",children:"Langchain-Beam"})," project."]}),"\n",(0,a.jsx)(n.p,{children:"Feel free to:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"\ud83d\udd27 Fork and customize the template to suit your specific use case, deploy it on your own infrastructure, or extend it to support new LLMs, embedding providers, or output formats."}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"\ud83c\udf31 Submit a PR if you'd like to contribute improvements, add new features, or even create entirely new   templates for other vector DBs or embedding models."}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"\ud83d\udc1e Create an issue if you run into any problems running the template on Flink, Dataflow, or locally \u2014 we're happy to help troubleshoot!"}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},6557:(e,n,t)=>{t.d(n,{A:()=>i});const i=t.p+"assets/images/kafka-tohelixdb-2893a9f5c0f764c12ef850e3703c5deb.png"},8453:(e,n,t)=>{t.d(n,{R:()=>l,x:()=>r});var i=t(6540);const a={},s=i.createContext(a);function l(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:l(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);