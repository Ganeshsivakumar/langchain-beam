"use strict";(globalThis.webpackChunkdocs_site=globalThis.webpackChunkdocs_site||[]).push([[136],{8453:(e,t,n)=>{n.d(t,{R:()=>i,x:()=>r});var a=n(6540);const s={},l=a.createContext(s);function i(e){const t=a.useContext(l);return a.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),a.createElement(l.Provider,{value:t},e.children)}},9096:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>o,contentTitle:()=>r,default:()=>h,frontMatter:()=>i,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"templates/llm-batch-process","title":"LLM batch processor","description":"Llm Batch Processor is a pre-built Apache Beam pipeline that lets you process a batch of text inputs using an LLM (OpenAI models) and save the results to a GCS path. You provide an instruction prompt that tells the model how to process the input data\u2014basically, what to do with it. The pipeline uses the model to transform the data and writes the final output to a GCS file.","source":"@site/docs/templates/llm-batch-process.md","sourceDirName":"templates","slug":"/templates/llm-batch-process","permalink":"/langchain-beam/docs/templates/llm-batch-process","draft":false,"unlisted":false,"editUrl":"https://github.com/Ganeshsivakumar/langchain-beam/tree/main/docs/docs-site/docs/docs/templates/llm-batch-process.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Kafka to Pinecone","permalink":"/langchain-beam/docs/templates/kafka-to-pinecone"},"next":{"title":"Integrations","permalink":"/langchain-beam/docs/category/integrations"}}');var s=n(4848),l=n(8453);const i={},r="LLM batch processor",o={},c=[{value:"What It Does \ud83d\udee0\ufe0f",id:"what-it-does-\ufe0f",level:2},{value:"Use Cases \ud83d\udce6",id:"use-cases-",level:2},{value:"Template Parameters \u2699\ufe0f",id:"template-parameters-\ufe0f",level:2},{value:"How to Run \ud83d\ude80",id:"how-to-run-",level:2},{value:"1. Google Cloud Dataflow",id:"1-google-cloud-dataflow",level:3},{value:"Run Template:",id:"run-template",level:4},{value:"Build Command:",id:"build-command",level:4},{value:"2. Apache Flink",id:"2-apache-flink",level:3},{value:"How It Works",id:"how-it-works",level:4},{value:"3. Locally",id:"3-locally",level:3},{value:"Run with Maven",id:"run-with-maven",level:4},{value:"4. Apache Spark",id:"4-apache-spark",level:3},{value:"Template Support \ud83e\uddf0",id:"template-support-",level:2}];function d(e){const t={a:"a",admonition:"admonition",br:"br",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,l.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(t.header,{children:(0,s.jsx)(t.h1,{id:"llm-batch-processor",children:"LLM batch processor"})}),"\n",(0,s.jsx)(t.p,{children:"Llm Batch Processor is a pre-built Apache Beam pipeline that lets you process a batch of text inputs using an LLM (OpenAI models) and save the results to a GCS path. You provide an instruction prompt that tells the model how to process the input data\u2014basically, what to do with it. The pipeline uses the model to transform the data and writes the final output to a GCS file."}),"\n",(0,s.jsx)(t.h2,{id:"what-it-does-\ufe0f",children:"What It Does \ud83d\udee0\ufe0f"}),"\n",(0,s.jsx)(t.p,{children:"This template:"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:"Reads input text data from a file (e.g., in Google Cloud Storage)."}),"\n",(0,s.jsxs)(t.li,{children:["Applies an LLM transformation via prompt using the ",(0,s.jsx)(t.a,{href:"https://github.com/Ganeshsivakumar/langchain-beam",children:"LangchainBeam"})," library."]}),"\n",(0,s.jsxs)(t.li,{children:["Writes the formatted output to a specified location in ",(0,s.jsx)(t.code,{children:".txt"})," format."]}),"\n"]}),"\n",(0,s.jsx)(t.h2,{id:"use-cases-",children:"Use Cases \ud83d\udce6"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:"Information extraction"}),"\n",(0,s.jsx)(t.li,{children:"Text summarization"}),"\n",(0,s.jsx)(t.li,{children:"Classification"}),"\n",(0,s.jsx)(t.li,{children:"Structured Output for Downstream Use"}),"\n"]}),"\n",(0,s.jsx)(t.h2,{id:"template-parameters-\ufe0f",children:"Template Parameters \u2699\ufe0f"}),"\n",(0,s.jsxs)(t.table,{children:[(0,s.jsx)(t.thead,{children:(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.th,{children:"Parameter"}),(0,s.jsx)(t.th,{children:"Description"})]})}),(0,s.jsxs)(t.tbody,{children:[(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"inputDataFile"})}),(0,s.jsxs)(t.td,{children:["Path to the input text file (e.g., ",(0,s.jsx)(t.code,{children:"gs://your-bucket/input.txt"}),")"]})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"llmOutputFile"})}),(0,s.jsxs)(t.td,{children:["Output file prefix (e.g., ",(0,s.jsx)(t.code,{children:"gs://your-bucket/output/llm-results"}),")"]})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"modelName"})}),(0,s.jsxs)(t.td,{children:["Name of the LLM model to use (e.g., ",(0,s.jsx)(t.code,{children:"gpt-3.5-turbo"}),", ",(0,s.jsx)(t.code,{children:"gpt-4o-mini"}),")"]})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"apiKey"})}),(0,s.jsxs)(t.td,{children:["Your OpenAI API key (e.g., ",(0,s.jsx)(t.code,{children:"sk-proj-BX_6MwMEV5_wont_share_my_key_tho"}),")"]})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"prompt"})}),(0,s.jsx)(t.td,{children:"Instruction prompt that guides the model on how to process the input data"})]})]})]}),"\n",(0,s.jsx)(t.h2,{id:"how-to-run-",children:"How to Run \ud83d\ude80"}),"\n",(0,s.jsx)(t.p,{children:"You can deploy this template using the Beam runner of your choice."}),"\n",(0,s.jsx)(t.h3,{id:"1-google-cloud-dataflow",children:"1. Google Cloud Dataflow"}),"\n",(0,s.jsxs)(t.p,{children:["This pipeline is built and packaged as a ",(0,s.jsx)(t.a,{href:"https://cloud.google.com/dataflow/docs/guides/templates/using-flex-templates",children:"Dataflow Flex Template"}),", and the template file (",(0,s.jsx)(t.code,{children:"gs://langbeam-cloud/templates/llm-batch-process.json"}),") is publicly accessible. This means you can run it directly in your own GCP project by using the ",(0,s.jsx)(t.code,{children:"gcloud"})," CLI with appropriate parameters."]}),"\n",(0,s.jsxs)(t.p,{children:["The pipeline source code is fully ",(0,s.jsx)(t.a,{href:"https://github.com/Ganeshsivakumar/langchain-beam",children:"open source on GitHub"}),". You're free to fork the repository, customize the pipeline, rebuild it as a Flex Template, and deploy it using your own GCP infrastructure."]}),"\n",(0,s.jsx)(t.h4,{id:"run-template",children:"Run Template:"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-bash",children:'gcloud dataflow flex-template run "llm-batch-process" \\\n  --template-file-gcs-location="gs://langbeam-cloud/templates/llm-batch-process.json" \\\n  --region="us-east1" \\\n  --project="ptransformers" \\\n  --network="default" \\\n  --subnetwork="https://www.googleapis.com/compute/v1/projects/project-id/regions/us-east1/subnetworks/default" \\\n  --staging-location="gs://your-stage-bucket/stage/" \\\n  --temp-location="gs://your-stage-bucket/temp/" \\\n  --parameters="inputDataFile=gs://pt-public-bucket/inputfile/product_reviews.csv,llmOutputFile=gs://your-bucket/outputfile/output,modelName=gpt-4o-mini,apiKey=openaikey,prompt=Categorize the product review as Positive or Negative"\n\n'})}),"\n",(0,s.jsx)(t.p,{children:"If you'd like to host the template in your own GCP project:"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Fork the Langchain-Beam repo"})," and clone it locally.",(0,s.jsx)(t.br,{}),"\n","You\u2019ll find the ",(0,s.jsx)(t.code,{children:"llm-batch-process"})," template under the ",(0,s.jsx)(t.code,{children:"templates/"})," directory."]}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Build the template"})," using Maven:"]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-bash",children:"mvn clean package -Prunner-dataflow\n"})}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:[(0,s.jsxs)(t.strong,{children:["Use the ",(0,s.jsx)(t.code,{children:"gcloud"})," CLI"]})," to run Dataflow Flex Template Build command.",(0,s.jsx)(t.br,{}),"\n","This will:"]}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:"Push your pipeline JAR as a container image to Artifact Registry"}),"\n",(0,s.jsxs)(t.li,{children:["Generate a ",(0,s.jsx)(t.code,{children:"template.json"})," file in your GCS bucket, which acts as a pointer to your pipeline image and contains configuration metadata"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(t.h4,{id:"build-command",children:"Build Command:"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-bash",children:"gcloud dataflow flex-template build gs://langbeam-cloud/templates/llm-batch-process.json \\\n  --image-gcr-path=us-docker.pkg.dev/your-projectid/your-repo/llm-batch-process:latest \\\n  --jar=/your-folder-path/langchain-beam/lbtemplate/langchain-beam/templates/llm-batch-process/target/llm-batch-process-dataflow.jar \\\n  --env=FLEX_TEMPLATE_JAVA_MAIN_CLASS=com.templates.langchainbeam.LlmBatchTextProcessor \\\n  --flex-template-base-image=JAVA17 \\\n  --metadata-file=/your-folder-path/langchain-beam/templates/llm-batch-process/src/main/metadata/metadata.json \\\n  --sdk-language=JAVA\n"})}),"\n",(0,s.jsx)(t.p,{children:"Now the template is buit and hosted your GCS path. you can pass GCS template file to run command to execute the template on dataflow"}),"\n",(0,s.jsx)(t.h3,{id:"2-apache-flink",children:"2. Apache Flink"}),"\n",(0,s.jsx)(t.p,{children:"If you have an Apache Flink standalone cluster, you can submit the template as a job using the prebuilt Docker image."}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-bash",children:'docker run --rm \\\n  -e FLINK_MASTER=host.docker.internal:8081 \\\n  -e FLINK_VERSION=1.18 \\\n  us-docker.pkg.dev/ptransformers/langbeam-cloud/templates/flink/llm-batch-process:latest \\\n  --runner=FlinkRunner \\\n  --inputDataFile=gs://pt-public-bucket/inputfile/product_reviews.csv \\\n  --llmOutputFile=gs://your-bucket/outputfile/output \\\n  --modelName=gpt-4o \\\n  --apiKey=your_openai_key \\\n  --prompt="Categorize the product review as Positive or Negative"\n'})}),"\n",(0,s.jsx)(t.h4,{id:"how-it-works",children:"How It Works"}),"\n",(0,s.jsxs)(t.p,{children:["The pipeline is built and packaged as a JAR. Since Apache Beam\u2019s Flink Runner must be compatible with your Flink version, there are multiple JARs available\u2014each tailored to a specific Beam and Flink version combination.\nRefer to the Flink version ",(0,s.jsx)(t.a,{href:"https://beam.apache.org/documentation/runners/flink/#flink-version-compatibility",children:"compatibility matrix"})," to choose the correct version."]}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:["The container downloads the appropriate .jar file from GCS based on ",(0,s.jsx)(t.em,{children:(0,s.jsx)(t.strong,{children:"FLINK_VERSION"})})," (your flink cluster version) with right beam and runner depedencies"]}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:["It uses the Flink CLI (flink run) to submit the job to the Flink cluster (as specified by ",(0,s.jsx)(t.code,{children:"FLINK_MASTER"}),")"]}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsx)(t.p,{children:"All dependencies\u2014including Java 17 and the Flink CLI\u2014are preinstalled in the image, so you don\u2019t need to set up anything else."}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(t.h3,{id:"3-locally",children:"3. Locally"}),"\n",(0,s.jsx)(t.p,{children:"You can also run the template locally using the Apache Beam DirectRunner. This is useful for testing, debugging, or running small jobs without setting up a full cluster."}),"\n",(0,s.jsx)(t.p,{children:"Prerequisites:"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:"JDK 17"}),"\n",(0,s.jsx)(t.li,{children:"Maven"}),"\n"]}),"\n",(0,s.jsx)(t.h4,{id:"run-with-maven",children:"Run with Maven"}),"\n",(0,s.jsx)(t.p,{children:"Clone the repository:"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-bash",children:"git clone https://github.com/Ganeshsivakumar/langchain-beam.git\ncd langchain-beam/templates/llm-batch-process\n"})}),"\n",(0,s.jsx)(t.p,{children:"Build the template:"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-bash",children:"mvn clean package -Prunner-direct\n"})}),"\n",(0,s.jsx)(t.p,{children:"Run with direct runner:"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-bash",children:'java -cp target/llm-batch-process-direct.jar \\\n  com.templates.langchainbeam.LlmBatchTextProcessor \\\n  --runner=DirectRunner \\\n  --inputDataFile=gs://pt-public-bucket/inputfile/product_reviews.csv \\\n  --llmOutputFile=/tmp/output \\\n  --modelName=gpt-4o \\\n  --apiKey=your_openai_key \\\n  --prompt="Categorize the product review as Positive or Negative"\n\n'})}),"\n",(0,s.jsx)(t.admonition,{type:"tip",children:(0,s.jsx)(t.p,{children:"You can use a local file (e.g., --inputDataFile=/path/to/input.txt) instead of GCS during local runs."})}),"\n",(0,s.jsx)(t.h3,{id:"4-apache-spark",children:"4. Apache Spark"}),"\n",(0,s.jsxs)(t.p,{children:["Spark support will be added soon via ",(0,s.jsx)(t.a,{href:"https://beam.apache.org/documentation/runners/spark/",children:"Beam's Spark runner"}),"."]}),"\n",(0,s.jsx)(t.h2,{id:"template-support-",children:"Template Support \ud83e\uddf0"}),"\n",(0,s.jsxs)(t.p,{children:["This template is fully open source and part of the ",(0,s.jsx)(t.a,{href:"https://github.com/Ganeshsivakumar/langchain-beam",children:"Langchain-Beam"})," project."]}),"\n",(0,s.jsx)(t.p,{children:"We built it to make it easier for developers and data teams to bring the power of LLMs into their data pipelines \u2014 whether you're using Flink, Dataflow, or running jobs locally."}),"\n",(0,s.jsx)(t.p,{children:"Feel free to:"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsx)(t.p,{children:"\ud83d\udd27 Fork and customize the template to suit your specific use case and Deploy it using your own infrastructure or extend it to support new LLMs or output formats."}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsx)(t.p,{children:"\ud83c\udf31 Submit a PR if you'd like to contribute improvements, new features, or even entirely new templates."}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsx)(t.p,{children:"\ud83d\udc1e Create an issue if you run into any problems running the template on Flink, Dataflow, or locally \u2014 we're happy to help troubleshoot!"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(t.p,{children:["\u2728 Want zero setup? A managed cloud platform is in the works. Join the ",(0,s.jsx)(t.a,{href:"https://app.youform.com/forms/9hbdmkly",children:"waitlist"})," or book a ",(0,s.jsx)(t.a,{href:"https://cal.com/ganesh-sivakumar/30min",children:"quick call"})," to chat about custom templates executed on LangBeam cloud platform."]})]})}function h(e={}){const{wrapper:t}={...(0,l.R)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}}}]);